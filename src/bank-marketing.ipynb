{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c3d546",
   "metadata": {},
   "source": [
    "# Bank Marketing Campaign Classification\n",
    "\n",
    "This notebook builds a classification pipeline to predict whether a customer will subscribe to a term deposit based on the UCI Bank Marketing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42effcd6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1ee09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zihanghuang/data-mining/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn: preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model interpretability\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2608781",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Configure project paths and load the Bank Marketing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6cddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/zihanghuang/data-mining\n",
      "Outputs directory: /Users/zihanghuang/data-mining/outputs\n"
     ]
    }
   ],
   "source": [
    "# Resolve project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"datasets\").exists() and (PROJECT_ROOT.parent / \"datasets\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if not (PROJECT_ROOT / \"datasets\").exists():\n",
    "    raise FileNotFoundError(\"Could not locate datasets folder. Run from project root or src directory.\")\n",
    "\n",
    "# Define directory structure\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" / \"bank-marketing\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "PREDICTIONS_DIR = OUTPUT_DIR / \"predictions\"\n",
    "\n",
    "# Create output directories\n",
    "for path in [PLOTS_DIR, MODELS_DIR, PREDICTIONS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Outputs directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d162f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45,211 rows and 17 columns\n",
      "\n",
      "Target distribution:\n",
      "y\n",
      "no     0.883015\n",
      "yes    0.116985\n",
      "\n",
      "Class imbalance ratio: 7.55:1\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = DATA_DIR / \"bank-full.csv\"\n",
    "df_raw = pd.read_csv(data_path, sep=\";\")\n",
    "\n",
    "print(f\"Loaded {df_raw.shape[0]:,} rows and {df_raw.shape[1]} columns\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_raw[\"y\"].value_counts(normalize=True).rename(\"proportion\").to_string())\n",
    "print(f\"\\nClass imbalance ratio: {df_raw['y'].value_counts()['no'] / df_raw['y'].value_counts()['yes']:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9b859",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Outlier Removal\n",
    "Remove extreme outliers (z-score > 3) from numeric columns to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 45,211\n",
      "Rows after outlier removal: 44,220\n",
      "Rows removed: 991 (2.2%)\n",
      "\n",
      "Note: 'duration' column dropped (only known post-call, causes data leakage)\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers using z-score method\n",
    "# Keeps more edge cases that may contain valuable signal\n",
    "df = df_raw.copy()\n",
    "outlier_cols = [\"balance\", \"age\", \"campaign\"]\n",
    "\n",
    "# Drop duration column - it's only known after a call ends (data leakage)\n",
    "df = df.drop(columns=[\"duration\"])\n",
    "\n",
    "# Create a combined mask for all outlier conditions (relaxed threshold)\n",
    "outlier_mask = pd.Series(True, index=df.index)\n",
    "for col in outlier_cols:\n",
    "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "    outlier_mask &= (z_scores < 4)  # Changed from 3 to 4 (less aggressive)\n",
    "\n",
    "df = df[outlier_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Rows before: {df_raw.shape[0]:,}\")\n",
    "print(f\"Rows after outlier removal: {df.shape[0]:,}\")\n",
    "print(f\"Rows removed: {df_raw.shape[0] - df.shape[0]:,} ({100 * (1 - df.shape[0] / df_raw.shape[0]):.1f}%)\")\n",
    "print(f\"\\nNote: 'duration' column dropped (only known post-call, causes data leakage)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d223511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features (9): ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
      "Numeric features (6): ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=\"y\")\n",
    "y = df[\"y\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gd8khc2c43d",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create domain-specific features to improve model predictive power.\n",
    "\n",
    "**Note**: The `duration` feature was removed from the dataset as it represents call duration, which is only known after a call ends and would cause data leakage in a real prediction scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "gkh9jw5auvi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 15\n",
      "Engineered features: 34\n",
      "New features added (19): ['balance_per_age', 'contact_intensity', 'contact_recency', 'contacted_and_success', 'has_high_balance', 'has_loan_or_default', 'has_positive_balance', 'high_campaign_effort', 'is_high_conversion_month', 'is_month_end', 'is_month_start', 'pdays_log', 'prev_failure', 'prev_success', 'prev_unknown', 'retired_age', 'total_contacts', 'was_contacted_before', 'young_single']\n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # -1 means never contacted, don't use 999 which creates outliers\n",
    "    df[\"was_contacted_before\"] = (df[\"pdays\"] != -1).astype(int)\n",
    "    \n",
    "    # For those contacted before, use log transform (adds 1 to avoid log(0))\n",
    "    # For never contacted, use median of contacted group\n",
    "    contacted_mask = df[\"pdays\"] != -1\n",
    "    if contacted_mask.sum() > 0:\n",
    "        median_pdays = df.loc[contacted_mask, \"pdays\"].median()\n",
    "        df[\"pdays_transformed\"] = df[\"pdays\"].replace(-1, median_pdays)\n",
    "        df[\"pdays_log\"] = np.log1p(df[\"pdays_transformed\"])\n",
    "    else:\n",
    "        df[\"pdays_log\"] = 0\n",
    "    \n",
    "    # Recency buckets, more informative than raw days\n",
    "    df[\"contact_recency\"] = pd.cut(\n",
    "        df[\"pdays\"].replace(-1, 9999),\n",
    "        bins=[-1, 7, 30, 90, 180, 365, 10000],\n",
    "        labels=[\"week\", \"month\", \"quarter\", \"half_year\", \"year\", \"never\"]\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Previous campaign success is extremely predictive (~65% conversion if success)\n",
    "    df[\"prev_success\"] = (df[\"poutcome\"] == \"success\").astype(int)\n",
    "    df[\"prev_failure\"] = (df[\"poutcome\"] == \"failure\").astype(int)\n",
    "    df[\"prev_unknown\"] = (df[\"poutcome\"] == \"unknown\").astype(int)\n",
    "    \n",
    "    # Interaction: contacted before AND had success\n",
    "    df[\"contacted_and_success\"] = (df[\"was_contacted_before\"] & df[\"prev_success\"]).astype(int)\n",
    "    \n",
    "    # === Contact history features ===\n",
    "    df[\"contact_intensity\"] = df[\"campaign\"] / (df[\"previous\"] + 1)\n",
    "    df[\"total_contacts\"] = df[\"campaign\"] + df[\"previous\"]\n",
    "    df[\"high_campaign_effort\"] = (df[\"campaign\"] > df[\"campaign\"].median()).astype(int)\n",
    "    \n",
    "    # === Financial features ===\n",
    "    df[\"balance_per_age\"] = df[\"balance\"] / (df[\"age\"] + 1)\n",
    "    df[\"has_positive_balance\"] = (df[\"balance\"] > 0).astype(int)\n",
    "    df[\"has_high_balance\"] = (df[\"balance\"] > df[\"balance\"].quantile(0.75)).astype(int)\n",
    "    df[\"has_loan_or_default\"] = ((df[\"loan\"] == \"yes\") | (df[\"default\"] == \"yes\")).astype(int)\n",
    "    \n",
    "    # === Demographic interactions ===\n",
    "    df[\"young_single\"] = ((df[\"age\"] < 30) & (df[\"marital\"] == \"single\")).astype(int)\n",
    "    df[\"retired_age\"] = (df[\"age\"] >= 60).astype(int)\n",
    "    \n",
    "    # === Time-based features ===\n",
    "    if \"day\" in df.columns:\n",
    "        df[\"is_month_start\"] = (df[\"day\"] <= 10).astype(int)\n",
    "        df[\"is_month_end\"] = (df[\"day\"] >= 20).astype(int)\n",
    "    \n",
    "    # Month seasonality (certain months have higher conversion)\n",
    "    high_conversion_months = [\"mar\", \"oct\", \"sep\", \"dec\"]\n",
    "    df[\"is_high_conversion_month\"] = df[\"month\"].isin(high_conversion_months).astype(int)\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df = df.drop(columns=[\"pdays_transformed\"], errors=\"ignore\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "X_engineered = engineer_features(X)\n",
    "\n",
    "new_features = set(X_engineered.columns) - set(X.columns)\n",
    "print(f\"Original features: {len(X.columns)}\")\n",
    "print(f\"Engineered features: {len(X_engineered.columns)}\")\n",
    "print(f\"New features added ({len(new_features)}): {sorted(new_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "h6i9yq5dnmi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: 10\n",
      "Numeric features: 24\n"
     ]
    }
   ],
   "source": [
    "# Update column lists for engineered features\n",
    "categorical_cols_eng = X_engineered.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols_eng = X_engineered.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols_eng),\n",
    "        (\"num\", StandardScaler(), numeric_cols_eng),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_cols_eng)}\")\n",
    "print(f\"Numeric features: {len(numeric_cols_eng)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hqwd3zkk6ur",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Use stratified split to preserve class distribution in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "i74flw68nsr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 26,532 samples (60%)\n",
      "Validation set: 8,844 samples (20%) - for early stopping\n",
      "Test set: 8,844 samples (20%)\n",
      "Positive class weight: 7.50\n",
      "\n",
      "Train class distribution:\n",
      "y\n",
      "no     0.882331\n",
      "yes    0.117669\n"
     ]
    }
   ],
   "source": [
    "# Stratified train/validation/test split\n",
    "# First split: train+val vs test (80/20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_engineered, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: train vs validation (75/25 of remaining = 60/20 overall)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.25,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Calculate class weight for imbalanced learning\n",
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples (60%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples (20%) - for early stopping\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples (20%)\")\n",
    "print(f\"Positive class weight: {pos_weight:.2f}\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts(normalize=True).rename({0: \"no\", 1: \"yes\"}).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8f563",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Utility functions for threshold optimization and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad7a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true: np.ndarray, y_proba: np.ndarray) -> tuple[float, float]:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # Calculate F1 for each threshold (avoid division by zero)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # Find best threshold (precision_recall_curve returns n+1 values, last has no threshold)\n",
    "    best_idx = np.argmax(f1_scores[:-1])\n",
    "    return thresholds[best_idx], f1_scores[best_idx]\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    name: str, \n",
    "    y_true: np.ndarray, \n",
    "    y_proba: np.ndarray, \n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    if threshold is None:\n",
    "        threshold, _ = find_optimal_threshold(y_true, y_proba)\n",
    "    \n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        \"name\": name,\n",
    "        \"threshold\": threshold,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_proba),\n",
    "        \"predictions\": y_pred,\n",
    "        \"probabilities\": y_proba,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} @ threshold {threshold:.3f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"  ROC AUC:   {metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac5eba",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train multiple classifiers with different approaches to handle class imbalance:\n",
    "1. **Class weights**: Built-in parameter to penalize misclassification of minority class\n",
    "2. **SMOTE**: Synthetic Minority Over-sampling Technique to balance training data\n",
    "\n",
    "### Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03860180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Logistic Regression @ threshold 0.649\n",
      "==================================================\n",
      "  Accuracy:  0.8703\n",
      "  Precision: 0.4498\n",
      "  Recall:    0.4563\n",
      "  F1 Score:  0.4530\n",
      "  ROC AUC:   0.7831\n"
     ]
    }
   ],
   "source": [
    "# Store all results for comparison\n",
    "all_results = []\n",
    "\n",
    "# Logistic Regression with class weights\n",
    "log_reg_pipeline = Pipeline([\n",
    "    (\"preprocessor\", clone(preprocessor)),\n",
    "    (\"classifier\", LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        class_weight=\"balanced\",\n",
    "        C=0.1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )),\n",
    "])\n",
    "\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "log_reg_proba = log_reg_pipeline.predict_proba(X_test)[:, 1]\n",
    "log_reg_results = evaluate_model(\"Logistic Regression\", y_test, log_reg_proba)\n",
    "all_results.append(log_reg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e1b30",
   "metadata": {},
   "source": [
    "### Manually implementation: Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23039b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DecisionTree as dt \n",
    "from tqdm import tqdm\n",
    "\n",
    "# find the best model\n",
    "max_depth_list = range(5,21,2)\n",
    "min_sample_list = range(5,21)\n",
    "best_evaluation_loss = 1e4\n",
    "best_max_depth = 0\n",
    "best_min_sample = 0\n",
    "best_preds = np.full_like(y_test,0.0,dtype=float)\n",
    "best_tree = None\n",
    "\n",
    "total_combinations = len(max_depth_list) * len(min_sample_list)\n",
    "with tqdm(total=total_combinations, desc=\"Grid Search\", unit=\"models\") as pbar:\n",
    "    for max_depth in max_depth_list:\n",
    "        for min_sample in min_sample_list:\n",
    "            pbar.set_postfix(max_depth=max_depth, min_sample=min_sample)\n",
    "            clstree = dt.ClassificationTree(loss_function=\"Entropy\", leaf_value_estimator=\"most_common_vote\", max_depth=max_depth, min_sample=min_sample)\n",
    "            clstree.fit(X_train, y_train)\n",
    "            clstree.prune(X_val, y_val)\n",
    "            y_preds = clstree.predict_batch(X_test)\n",
    "            current_evaluation_loss = np.sum(y_preds != y_test)\n",
    "            if current_evaluation_loss < best_evaluation_loss:\n",
    "                best_evaluation_loss = current_evaluation_loss\n",
    "                best_max_depth = max_depth \n",
    "                best_min_sample = min_sample\n",
    "                best_preds = y_preds\n",
    "                best_tree = clstree\n",
    "            pbar.update(1)\n",
    "\n",
    "print(f\"\\nBest parameters: max_depth={best_max_depth}, min_sample={best_min_sample}\")\n",
    "print(f\"Best evaluation loss: {best_evaluation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec369ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clstree.prune(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = clstree.predict_batch(X_test)\n",
    "evaluate_model(\"Classificationtree\", y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w71qi00vuxe",
   "metadata": {},
   "source": [
    "### XGBoost with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h6bhlqiwemw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with SMOTE and early stopping\n",
    "\n",
    "preprocessor_fitted = clone(preprocessor).fit(X_train)\n",
    "X_train_transformed = preprocessor_fitted.transform(X_train)\n",
    "X_val_transformed = preprocessor_fitted.transform(X_val)\n",
    "X_test_transformed = preprocessor_fitted.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "print(f\"Training samples before SMOTE: {X_train_transformed.shape[0]:,}\")\n",
    "print(f\"Training samples after SMOTE: {X_train_smote.shape[0]:,}\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=1000,  # High value, early stopping will find optimal\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.15,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.5,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",  # Use AUC for imbalanced data\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=\"hist\",\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train_smote, y_train_smote,\n",
    "    eval_set=[(X_val_transformed, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "# Wrapper pipeline for consistency\n",
    "class XGBSMOTEWrapper:\n",
    "    def __init__(self, preprocessor, model):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_transformed = self.preprocessor.transform(X)\n",
    "        return self.model.predict_proba(X_transformed)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_transformed = self.preprocessor.transform(X)\n",
    "        return self.model.predict(X_transformed)\n",
    "\n",
    "xgb_smote_pipeline = XGBSMOTEWrapper(preprocessor_fitted, xgb_model)\n",
    "\n",
    "# Evaluate\n",
    "xgb_smote_proba = xgb_smote_pipeline.predict_proba(X_test)[:, 1]\n",
    "xgb_smote_results = evaluate_model(\"XGBoost + SMOTE\", y_test, xgb_smote_proba)\n",
    "all_results.append(xgb_smote_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nwbk8i29n6",
   "metadata": {},
   "source": [
    "### Random Forest with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3ew4vn7l4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with SMOTE\n",
    "# Reuse transformed and SMOTE'd data from XGBoost cell\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=12,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=\"balanced\",  # Additional class weighting on top of SMOTE\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "rf_smote_pipeline = XGBSMOTEWrapper(preprocessor_fitted, rf_model)\n",
    "\n",
    "rf_smote_proba = rf_smote_pipeline.predict_proba(X_test)[:, 1]\n",
    "rf_smote_results = evaluate_model(\"Random Forest + SMOTE\", y_test, rf_smote_proba)\n",
    "all_results.append(rf_smote_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bax69h1xs5r",
   "metadata": {},
   "source": [
    "### LightGBM with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wlkx26l2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with SMOTE and early stopping\n",
    "# Reuse transformed data from XGBoost cell\n",
    "\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=1000,  # High value, early stopping will find optimal\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1,\n",
    ")\n",
    "\n",
    "lgbm_model.fit(\n",
    "    X_train_smote, y_train_smote,\n",
    "    eval_set=[(X_val_transformed, y_val)],\n",
    "    callbacks=[early_stopping(50, verbose=False), log_evaluation(period=0)],\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {lgbm_model.best_iteration_}\")\n",
    "\n",
    "lgbm_smote_pipeline = XGBSMOTEWrapper(preprocessor_fitted, lgbm_model)\n",
    "\n",
    "lgbm_smote_proba = lgbm_smote_pipeline.predict_proba(X_test)[:, 1]\n",
    "lgbm_smote_results = evaluate_model(\"LightGBM + SMOTE\", y_test, lgbm_smote_proba)\n",
    "all_results.append(lgbm_smote_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f168ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with class weights (scale_pos_weight)\n",
    "xgb_weighted_pipeline = Pipeline([\n",
    "    (\"preprocessor\", clone(preprocessor)),\n",
    "    (\"classifier\", XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.15,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.5,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        scale_pos_weight=pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "xgb_weighted_pipeline.fit(X_train, y_train)\n",
    "xgb_weighted_proba = xgb_weighted_pipeline.predict_proba(X_test)[:, 1]\n",
    "xgb_weighted_results = evaluate_model(\"XGBoost (weighted)\", y_test, xgb_weighted_proba)\n",
    "all_results.append(xgb_weighted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jnfufql5r3",
   "source": "### Reactive Rule Learner (RRL)\n\nRRL is an interpretable rule-based model that learns logical rules from data. It provides transparent decision-making through human-readable rules.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "otxrk7hydn",
   "source": "# RRL Training using preprocessed and engineered features\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom rrl.models import RRL\nfrom rrl.utils import DBEncoder\n\n# Set device for RRL\nrrl_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntry:\n    if torch.backends.mps.is_available():\n        rrl_device = torch.device(\"mps\")\nexcept:\n    pass\nprint(f\"Using device for RRL: {rrl_device}\")\n\n# Use engineered features (same as other models)\n# Create feature info dataframe for DBEncoder from X_engineered\nprint(\"Preparing engineered features for RRL...\")\nf_list = []\nfor col in X_engineered.columns:\n    if X_engineered[col].dtype == 'object':\n        f_list.append([col, 'discrete'])\n    else:\n        f_list.append([col, 'continuous'])\n\nf_df = pd.DataFrame(f_list)\n\n# Initialize DBEncoder with engineered features\ndb_enc = DBEncoder(f_df, discrete=False)\n\n# Create target dataframe (DBEncoder expects DataFrame)\ny_train_df = y_train.to_frame(name='y')\ny_val_df = y_val.to_frame(name='y')\ny_test_df = y_test.to_frame(name='y')\n\n# Fit encoder on training data\ndb_enc.fit(X_train, y_train_df)\n\n# Transform all splits\nX_rrl_train, y_rrl_train = db_enc.transform(X_train, y_train_df, normalized=True, keep_stat=True)\nX_rrl_val, y_rrl_val = db_enc.transform(X_val, y_val_df, normalized=True, keep_stat=False)\nX_rrl_test, y_rrl_test = db_enc.transform(X_test, y_test_df, normalized=True, keep_stat=False)\n\nprint(f\"RRL Training samples: {X_rrl_train.shape[0]:,}\")\nprint(f\"RRL Validation samples: {X_rrl_val.shape[0]:,}\")\nprint(f\"RRL Test samples: {X_rrl_test.shape[0]:,}\")\n\n# Create DataLoaders\nrrl_batch_size = 64\nrrl_train_dataset = TensorDataset(\n    torch.tensor(X_rrl_train, dtype=torch.float32),\n    torch.tensor(y_rrl_train, dtype=torch.float32)\n)\nrrl_val_dataset = TensorDataset(\n    torch.tensor(X_rrl_val, dtype=torch.float32),\n    torch.tensor(y_rrl_val, dtype=torch.float32)\n)\nrrl_test_dataset = TensorDataset(\n    torch.tensor(X_rrl_test, dtype=torch.float32),\n    torch.tensor(y_rrl_test, dtype=torch.float32)\n)\n\nrrl_train_loader = DataLoader(rrl_train_dataset, batch_size=rrl_batch_size, shuffle=True)\nrrl_val_loader = DataLoader(rrl_val_dataset, batch_size=rrl_batch_size, shuffle=False)\nrrl_test_loader = DataLoader(rrl_test_dataset, batch_size=rrl_batch_size, shuffle=False)\n\n# Initialize RRL Model\ndiscrete_flen = db_enc.discrete_flen\ncontinuous_flen = db_enc.continuous_flen\nrrl_output_dim = y_rrl_train.shape[1]  # One-hot encoded length\n\n# Structure: [16, 16, 8] for the dataset complexity\nstructure = [16, 16, 8]\ndim_list = [(discrete_flen, continuous_flen)] + structure + [rrl_output_dim]\n\nprint(f\"RRL Structure: {dim_list}\")\nprint(f\"Discrete features: {discrete_flen}, Continuous features: {continuous_flen}\")\n\nrrl_model_path = MODELS_DIR / 'rrl_bank_model.pth'\nrrl_log_path = MODELS_DIR / 'rrl_bank_log.txt'\n\nrrl_model = RRL(\n    dim_list=dim_list,\n    device=rrl_device,\n    use_not=True,\n    is_rank0=True,\n    save_best=True,\n    distributed=False,\n    save_path=str(rrl_model_path),\n    log_file=str(rrl_log_path)\n)\n\n# Train with validation set for early stopping\nprint(\"Starting RRL training...\")\nrrl_model.train_model(\n    data_loader=rrl_train_loader,\n    valid_loader=rrl_val_loader,\n    epoch=20,\n    lr=0.01\n)\n\n# Test\nprint(\"Testing RRL model...\")\nrrl_model.test(test_loader=rrl_test_loader, set_name='Test')\n\n# Print Rules\nprint(\"Generating rules...\")\nrrl_rules_path = MODELS_DIR / 'rrl_bank_rules.txt'\nwith open(rrl_rules_path, 'w') as f:\n    rrl_model.rule_print(db_enc.X_fname, db_enc.y_fname, rrl_train_loader, file=f, mean=db_enc.mean, std=db_enc.std)\n\nprint(f\"RRL Model saved to {rrl_model_path}\")\nprint(f\"RRL Rules saved to {rrl_rules_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "liyf90z1f",
   "source": "### RRL Evaluation\n\nLoad the trained RRL model and evaluate using the standard `evaluate_model` function for comparison with other models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zmtq1ndr1cm",
   "source": "# RRL Evaluation - Get probabilities for evaluate_model function\n# Uses the same test split as other models (y_test)\nprint(f\"Loading RRL model from {rrl_model_path}...\")\n\nif not rrl_model_path.exists():\n    print(f\"Error: Model file not found at {rrl_model_path}\")\nelse:\n    checkpoint = torch.load(rrl_model_path, map_location=rrl_device, weights_only=False)\n    saved_args = checkpoint['rrl_args']\n    model_state_dict = checkpoint['model_state_dict']\n\n    # Re-instantiate RRL model with saved arguments\n    rrl_eval = RRL(\n        dim_list=saved_args['dim_list'],\n        device=rrl_device,\n        use_not=saved_args['use_not'],\n        use_skip=saved_args.get('use_skip', False),\n        estimated_grad=saved_args.get('estimated_grad', False),\n        use_nlaf=saved_args.get('use_nlaf', False),\n        alpha=saved_args.get('alpha', 0.999),\n        beta=saved_args.get('beta', 8),\n        gamma=saved_args.get('gamma', 1),\n        distributed=False,\n        is_rank0=True\n    )\n\n    # Load weights (handle potential 'module.' prefix from DistributedDataParallel)\n    new_state_dict = {}\n    for k, v in model_state_dict.items():\n        name = k[7:] if k.startswith('module.') else k\n        new_state_dict[name] = v\n\n    rrl_eval.net.load_state_dict(new_state_dict)\n    rrl_eval.net.eval()\n    print(\"RRL Model loaded successfully.\")\n\n    # Collect predictions and probabilities using the same test loader\n    rrl_y_proba_list = []\n\n    with torch.no_grad():\n        for X_batch, y_batch in rrl_test_loader:\n            X_batch = X_batch.to(rrl_device)\n            \n            # Forward pass\n            outputs = rrl_eval.net(X_batch)\n            \n            # Apply softmax to get probabilities\n            probs = torch.softmax(outputs, dim=1)\n            \n            rrl_y_proba_list.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (yes)\n\n    rrl_y_proba = np.array(rrl_y_proba_list)\n\n    # Evaluate using y_test (same as other models) for consistent comparison\n    rrl_results = evaluate_model(\"RRL\", y_test.values, rrl_y_proba)\n    all_results.append(rrl_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lw0yjuc82h",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare all models across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876rfj9rzsw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {k: v for k, v in r.items() if k not in [\"predictions\", \"probabilities\"]}\n",
    "    for r in all_results\n",
    "]).sort_values(\"f1\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON (sorted by F1 score)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0][\"name\"]\n",
    "best_f1 = comparison_df.iloc[0][\"f1\"]\n",
    "print(f\"\\nBest model: {best_model_name} (F1 = {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oltxqc5nzro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "metrics_to_plot = [\"f1\", \"precision\", \"recall\"]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(comparison_df)))\n",
    "\n",
    "for ax, metric in zip(axes, metrics_to_plot):\n",
    "    sorted_df = comparison_df.sort_values(metric, ascending=True)\n",
    "    bars = ax.barh(sorted_df[\"name\"], sorted_df[metric], color=colors)\n",
    "    ax.set_xlabel(metric.capitalize())\n",
    "    ax.set_title(f\"{metric.capitalize()} by Model\")\n",
    "    ax.set_xlim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, sorted_df[metric]):\n",
    "        ax.text(val + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                f\"{val:.3f}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_plot_path = PLOTS_DIR / \"model_comparison.png\"\n",
    "plt.savefig(comparison_plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {comparison_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823720cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, title: str, ax=None):\n",
    "    \"\"\"Plot a confusion matrix with annotations.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "        xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"]\n",
    "    )\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Plot confusion matrices for all models in a grid\n",
    "n_models = len(all_results)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    title = f\"{result['name']}\\n(thr={result['threshold']:.3f})\"\n",
    "    plot_confusion_matrix(y_test, result[\"predictions\"], title, ax=axes[idx])\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(all_results), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "cm_plot_path = PLOTS_DIR / \"confusion_matrices.png\"\n",
    "plt.savefig(cm_plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {cm_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7cb9e",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map model names to pipelines\n",
    "pipelines = {\n",
    "    \"Logistic Regression\": log_reg_pipeline,\n",
    "    \"XGBoost (weighted)\": xgb_weighted_pipeline,\n",
    "    \"XGBoost + SMOTE\": xgb_smote_pipeline,\n",
    "    \"Random Forest + SMOTE\": rf_smote_pipeline,\n",
    "    \"LightGBM + SMOTE\": lgbm_smote_pipeline,\n",
    "}\n",
    "\n",
    "# Save all models with their metadata\n",
    "for result in all_results:\n",
    "    model_name = result[\"name\"]\n",
    "    safe_name = model_name.lower().replace(\" \", \"_\").replace(\"+\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    pipeline = pipelines[model_name]\n",
    "    \n",
    "    # Handle wrapper objects vs sklearn pipelines\n",
    "    if hasattr(pipeline, 'named_steps'):\n",
    "        model_to_save = pipeline\n",
    "    else:\n",
    "        # For wrapper objects, save the components\n",
    "        model_to_save = {\n",
    "            \"preprocessor\": pipeline.preprocessor,\n",
    "            \"model\": pipeline.model,\n",
    "            \"is_wrapper\": True\n",
    "        }\n",
    "    \n",
    "    model_data = {\n",
    "        \"pipeline\": model_to_save,\n",
    "        \"threshold\": result[\"threshold\"],\n",
    "        \"metrics\": {k: v for k, v in result.items() if k not in [\"predictions\", \"probabilities\"]},\n",
    "    }\n",
    "    \n",
    "    model_path = MODELS_DIR / f\"{safe_name}.pkl\"\n",
    "    joblib.dump(model_data, model_path)\n",
    "    print(f\"Saved: {model_path}\")\n",
    "\n",
    "print(f\"\\nBest model by F1 score: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10e38c",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "Export predictions from all models for analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac88c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile predictions from all models\n",
    "predictions_df = pd.DataFrame({\"actual\": y_test})\n",
    "\n",
    "for result in all_results:\n",
    "    safe_name = result[\"name\"].lower().replace(\" \", \"_\").replace(\"+\", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    predictions_df[f\"{safe_name}_prob\"] = result[\"probabilities\"]\n",
    "    predictions_df[f\"{safe_name}_pred\"] = result[\"predictions\"]\n",
    "\n",
    "# Save predictions\n",
    "predictions_path = PREDICTIONS_DIR / \"classification_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions: {predictions_path}\")\n",
    "print(f\"Shape: {predictions_df.shape}\")\n",
    "print(f\"\\nColumns: {list(predictions_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021071",
   "metadata": {},
   "source": [
    "## Model Interpretability\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to understand feature importance for the best tree-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ea3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best tree-based model for SHAP analysis\n",
    "tree_models = comparison_df[comparison_df[\"name\"].str.contains(\"XGBoost|LightGBM|Random Forest\")]\n",
    "best_tree_model = tree_models.iloc[0][\"name\"]\n",
    "\n",
    "pipeline_map = {\n",
    "    \"XGBoost (weighted)\": xgb_weighted_pipeline,\n",
    "    \"XGBoost + SMOTE\": xgb_smote_pipeline,\n",
    "    \"Random Forest + SMOTE\": rf_smote_pipeline,\n",
    "    \"LightGBM + SMOTE\": lgbm_smote_pipeline,\n",
    "}\n",
    "\n",
    "shap_pipeline = pipeline_map[best_tree_model]\n",
    "print(f\"Generating SHAP values for: {best_tree_model}\")\n",
    "\n",
    "# Get feature names and classifier based on pipeline type\n",
    "if hasattr(shap_pipeline, 'named_steps'):\n",
    "    # Standard sklearn pipeline\n",
    "    feature_names = shap_pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "    X_test_transformed = shap_pipeline.named_steps[\"preprocessor\"].transform(X_test)\n",
    "    classifier = shap_pipeline.named_steps[\"classifier\"]\n",
    "else:\n",
    "    # Wrapper object\n",
    "    feature_names = shap_pipeline.preprocessor.get_feature_names_out()\n",
    "    X_test_transformed = shap_pipeline.preprocessor.transform(X_test)\n",
    "    classifier = shap_pipeline.model\n",
    "\n",
    "# Sample for faster computation\n",
    "sample_size = min(1000, X_test_transformed.shape[0])\n",
    "sample_idx = np.random.RandomState(RANDOM_STATE).choice(\n",
    "    X_test_transformed.shape[0], sample_size, replace=False\n",
    ")\n",
    "X_sample = X_test_transformed[sample_idx]\n",
    "\n",
    "# Convert sparse matrix to dense if needed\n",
    "if hasattr(X_sample, \"toarray\"):\n",
    "    X_sample = X_sample.toarray()\n",
    "\n",
    "# Create SHAP explainer and compute values\n",
    "explainer = shap.TreeExplainer(classifier)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Handle binary classification output format\n",
    "# Some models return list [neg_class, pos_class], others return single array\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_display = shap_values[1]  # Use positive class\n",
    "else:\n",
    "    shap_values_display = shap_values\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values_display.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xtll6lewh0i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary (bar chart)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values_display,\n",
    "    X_sample,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    max_display=15,\n",
    "    show=False,\n",
    ")\n",
    "plt.title(f\"Feature Importance - {best_tree_model}\")\n",
    "plt.tight_layout()\n",
    "shap_bar_path = PLOTS_DIR / \"shap_feature_importance.png\"\n",
    "plt.savefig(shap_bar_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {shap_bar_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vvdaf4txmmj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary (beeswarm - shows feature value impact)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values_display,\n",
    "    X_sample,\n",
    "    feature_names=feature_names,\n",
    "    max_display=15,\n",
    "    show=False,\n",
    ")\n",
    "plt.title(f\"SHAP Value Distribution - {best_tree_model}\")\n",
    "plt.tight_layout()\n",
    "shap_beeswarm_path = PLOTS_DIR / \"shap_beeswarm.png\"\n",
    "plt.savefig(shap_beeswarm_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {shap_beeswarm_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c3d546",
   "metadata": {},
   "source": [
    "# Bank Marketing Campaign Classification\n",
    "\n",
    "This notebook builds a classification pipeline to predict whether a customer will subscribe to a term deposit based on the UCI Bank Marketing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42effcd6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ee09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn: preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model interpretability\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2608781",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Configure project paths and load the Bank Marketing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6cddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve project paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"datasets\").exists() and (PROJECT_ROOT.parent / \"datasets\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if not (PROJECT_ROOT / \"datasets\").exists():\n",
    "    raise FileNotFoundError(\"Could not locate datasets folder. Run from project root or src directory.\")\n",
    "\n",
    "# Define directory structure\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" / \"bank-marketing\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "PREDICTIONS_DIR = OUTPUT_DIR / \"predictions\"\n",
    "\n",
    "# Create output directories\n",
    "for path in [PLOTS_DIR, MODELS_DIR, PREDICTIONS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Outputs directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = DATA_DIR / \"bank-full.csv\"\n",
    "df_raw = pd.read_csv(data_path, sep=\";\")\n",
    "\n",
    "print(f\"Loaded {df_raw.shape[0]:,} rows and {df_raw.shape[1]} columns\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_raw[\"y\"].value_counts(normalize=True).rename(\"proportion\").to_string())\n",
    "print(f\"\\nClass imbalance ratio: {df_raw['y'].value_counts()['no'] / df_raw['y'].value_counts()['yes']:.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9b859",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Outlier Removal\n",
    "Remove extreme outliers (z-score > 3) from numeric columns to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers using z-score method\n",
    "# Keeps more edge cases that may contain valuable signal\n",
    "df = df_raw.copy()\n",
    "outlier_cols = [\"balance\", \"age\", \"campaign\"]\n",
    "\n",
    "# Drop duration column - it's only known after a call ends (data leakage)\n",
    "df = df.drop(columns=[\"duration\"])\n",
    "\n",
    "# Create a combined mask for all outlier conditions (relaxed threshold)\n",
    "outlier_mask = pd.Series(True, index=df.index)\n",
    "for col in outlier_cols:\n",
    "    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "    outlier_mask &= (z_scores < 4)  # Changed from 3 to 4 (less aggressive)\n",
    "\n",
    "df = df[outlier_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Rows before: {df_raw.shape[0]:,}\")\n",
    "print(f\"Rows after outlier removal: {df.shape[0]:,}\")\n",
    "print(f\"Rows removed: {df_raw.shape[0] - df.shape[0]:,} ({100 * (1 - df.shape[0] / df_raw.shape[0]):.1f}%)\")\n",
    "print(f\"\\nNote: 'duration' column dropped (only known post-call, causes data leakage)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d223511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=\"y\")\n",
    "y = df[\"y\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numeric features ({len(numeric_cols)}): {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gd8khc2c43d",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create domain-specific features to improve model predictive power.\n",
    "\n",
    "**Note**: The `duration` feature was removed from the dataset as it represents call duration, which is only known after a call ends and would cause data leakage in a real prediction scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gkh9jw5auvi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    # -1 means never contacted, don't use 999 which creates outliers\n",
    "    df[\"was_contacted_before\"] = (df[\"pdays\"] != -1).astype(int)\n",
    "    \n",
    "    # For those contacted before, use log transform (adds 1 to avoid log(0))\n",
    "    # For never contacted, use median of contacted group\n",
    "    contacted_mask = df[\"pdays\"] != -1\n",
    "    if contacted_mask.sum() > 0:\n",
    "        median_pdays = df.loc[contacted_mask, \"pdays\"].median()\n",
    "        df[\"pdays_transformed\"] = df[\"pdays\"].replace(-1, median_pdays)\n",
    "        df[\"pdays_log\"] = np.log1p(df[\"pdays_transformed\"])\n",
    "    else:\n",
    "        df[\"pdays_log\"] = 0\n",
    "    \n",
    "    # Recency buckets, more informative than raw days\n",
    "    df[\"contact_recency\"] = pd.cut(\n",
    "        df[\"pdays\"].replace(-1, 9999),\n",
    "        bins=[-1, 7, 30, 90, 180, 365, 10000],\n",
    "        labels=[\"week\", \"month\", \"quarter\", \"half_year\", \"year\", \"never\"]\n",
    "    ).astype(str)\n",
    "    \n",
    "    # Previous campaign success is extremely predictive (~65% conversion if success)\n",
    "    df[\"prev_success\"] = (df[\"poutcome\"] == \"success\").astype(int)\n",
    "    df[\"prev_failure\"] = (df[\"poutcome\"] == \"failure\").astype(int)\n",
    "    df[\"prev_unknown\"] = (df[\"poutcome\"] == \"unknown\").astype(int)\n",
    "    \n",
    "    # Interaction: contacted before AND had success\n",
    "    df[\"contacted_and_success\"] = (df[\"was_contacted_before\"] & df[\"prev_success\"]).astype(int)\n",
    "    \n",
    "    # === Contact history features ===\n",
    "    df[\"contact_intensity\"] = df[\"campaign\"] / (df[\"previous\"] + 1)\n",
    "    df[\"total_contacts\"] = df[\"campaign\"] + df[\"previous\"]\n",
    "    df[\"high_campaign_effort\"] = (df[\"campaign\"] > df[\"campaign\"].median()).astype(int)\n",
    "    \n",
    "    # === Financial features ===\n",
    "    df[\"balance_per_age\"] = df[\"balance\"] / (df[\"age\"] + 1)\n",
    "    df[\"has_positive_balance\"] = (df[\"balance\"] > 0).astype(int)\n",
    "    df[\"has_high_balance\"] = (df[\"balance\"] > df[\"balance\"].quantile(0.75)).astype(int)\n",
    "    df[\"has_loan_or_default\"] = ((df[\"loan\"] == \"yes\") | (df[\"default\"] == \"yes\")).astype(int)\n",
    "    \n",
    "    # === Demographic interactions ===\n",
    "    df[\"young_single\"] = ((df[\"age\"] < 30) & (df[\"marital\"] == \"single\")).astype(int)\n",
    "    df[\"retired_age\"] = (df[\"age\"] >= 60).astype(int)\n",
    "    \n",
    "    # === Time-based features ===\n",
    "    if \"day\" in df.columns:\n",
    "        df[\"is_month_start\"] = (df[\"day\"] <= 10).astype(int)\n",
    "        df[\"is_month_end\"] = (df[\"day\"] >= 20).astype(int)\n",
    "    \n",
    "    # Month seasonality (certain months have higher conversion)\n",
    "    high_conversion_months = [\"mar\", \"oct\", \"sep\", \"dec\"]\n",
    "    df[\"is_high_conversion_month\"] = df[\"month\"].isin(high_conversion_months).astype(int)\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df = df.drop(columns=[\"pdays_transformed\"], errors=\"ignore\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply feature engineering\n",
    "X_engineered = engineer_features(X)\n",
    "\n",
    "new_features = set(X_engineered.columns) - set(X.columns)\n",
    "print(f\"Original features: {len(X.columns)}\")\n",
    "print(f\"Engineered features: {len(X_engineered.columns)}\")\n",
    "print(f\"New features added ({len(new_features)}): {sorted(new_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h6i9yq5dnmi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column lists for engineered features\n",
    "categorical_cols_eng = X_engineered.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols_eng = X_engineered.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols_eng),\n",
    "        (\"num\", StandardScaler(), numeric_cols_eng),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Categorical features: {len(categorical_cols_eng)}\")\n",
    "print(f\"Numeric features: {len(numeric_cols_eng)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hqwd3zkk6ur",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "\n",
    "Use stratified split to preserve class distribution in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i74flw68nsr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/validation/test split\n",
    "# First split: train+val vs test (80/20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_engineered, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: train vs validation (75/25 of remaining = 60/20 overall)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.25,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Calculate class weight for imbalanced learning\n",
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples (60%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples (20%) - for early stopping\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples (20%)\")\n",
    "print(f\"Positive class weight: {pos_weight:.2f}\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts(normalize=True).rename({0: \"no\", 1: \"yes\"}).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8f563",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Utility functions for threshold optimization and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true: np.ndarray, y_proba: np.ndarray) -> tuple[float, float]:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # Calculate F1 for each threshold (avoid division by zero)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    \n",
    "    # Find best threshold (precision_recall_curve returns n+1 values, last has no threshold)\n",
    "    best_idx = np.argmax(f1_scores[:-1])\n",
    "    return thresholds[best_idx], f1_scores[best_idx]\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    name: str, \n",
    "    y_true: np.ndarray, \n",
    "    y_proba: np.ndarray, \n",
    "    threshold: float = None\n",
    ") -> dict:\n",
    "    if threshold is None:\n",
    "        threshold, _ = find_optimal_threshold(y_true, y_proba)\n",
    "    \n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        \"name\": name,\n",
    "        \"threshold\": threshold,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_proba),\n",
    "        \"predictions\": y_pred,\n",
    "        \"probabilities\": y_proba,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} @ threshold {threshold:.3f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"  ROC AUC:   {metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac5eba",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train multiple classifiers with different approaches to handle class imbalance:\n",
    "1. **Class weights**: Built-in parameter to penalize misclassification of minority class\n",
    "2. **SMOTE**: Synthetic Minority Over-sampling Technique to balance training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03860180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results for comparison\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e1b30",
   "metadata": {},
   "source": [
    "### Manually implementation: Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23039b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DecisionTree as dt \n",
    "from tqdm import tqdm\n",
    "\n",
    "# find the best model\n",
    "# max_depth_list = range(5,21,2)\n",
    "# min_sample_list = range(5,21)\n",
    "# best_evaluation_loss = 1e4\n",
    "# best_max_depth = 0\n",
    "# best_min_sample = 0\n",
    "# best_preds = np.full_like(y_test,0.0,dtype=float)\n",
    "# best_tree = None\n",
    "\n",
    "# total_combinations = len(max_depth_list) * len(min_sample_list)\n",
    "# with tqdm(total=total_combinations, desc=\"Grid Search\", unit=\"models\") as pbar:\n",
    "#     for max_depth in max_depth_list:\n",
    "#         for min_sample in min_sample_list:\n",
    "#             pbar.set_postfix(max_depth=max_depth, min_sample=min_sample)\n",
    "#             clstree = dt.ClassificationTree(loss_function=\"Entropy\", leaf_value_estimator=\"most_common_vote\", max_depth=max_depth, min_sample=min_sample)\n",
    "#             clstree.fit(X_train, y_train)\n",
    "#             clstree.prune(X_val, y_val)\n",
    "#             y_preds = clstree.predict_batch(X_test)\n",
    "#             current_evaluation_loss = np.sum(y_preds != y_test)\n",
    "#             if current_evaluation_loss < best_evaluation_loss:\n",
    "#                 best_evaluation_loss = current_evaluation_loss\n",
    "#                 best_max_depth = max_depth \n",
    "#                 best_min_sample = min_sample\n",
    "#                 best_preds = y_preds\n",
    "#                 best_tree = clstree\n",
    "#             pbar.update(1)\n",
    "\n",
    "# print(f\"\\nBest parameters: max_depth={best_max_depth}, min_sample={best_min_sample}\")\n",
    "# print(f\"Best evaluation loss: {best_evaluation_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec369ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clstree.prune(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_preds = clstree.predict_batch(X_test)\n",
    "# evaluate_model(\"Classificationtree\", y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w71qi00vuxe",
   "metadata": {},
   "source": [
    "### XGBoost with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h6bhlqiwemw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with SMOTE and early stopping\n",
    "\n",
    "preprocessor_fitted = clone(preprocessor).fit(X_train)\n",
    "X_train_transformed = preprocessor_fitted.transform(X_train)\n",
    "X_val_transformed = preprocessor_fitted.transform(X_val)\n",
    "X_test_transformed = preprocessor_fitted.transform(X_test)\n",
    "\n",
    "smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "print(f\"Training samples before SMOTE: {X_train_transformed.shape[0]:,}\")\n",
    "print(f\"Training samples after SMOTE: {X_train_smote.shape[0]:,}\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=1000,  # High value, early stopping will find optimal\n",
    "    learning_rate=0.03,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.15,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.5,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",  # Use AUC for imbalanced data\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method=\"hist\",\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train_smote, y_train_smote,\n",
    "    eval_set=[(X_val_transformed, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "# Wrapper pipeline for consistency\n",
    "class XGBSMOTEWrapper:\n",
    "    def __init__(self, preprocessor, model):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_transformed = self.preprocessor.transform(X)\n",
    "        return self.model.predict_proba(X_transformed)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_transformed = self.preprocessor.transform(X)\n",
    "        return self.model.predict(X_transformed)\n",
    "\n",
    "xgb_smote_pipeline = XGBSMOTEWrapper(preprocessor_fitted, xgb_model)\n",
    "\n",
    "# Evaluate\n",
    "xgb_smote_proba = xgb_smote_pipeline.predict_proba(X_test)[:, 1]\n",
    "xgb_smote_results = evaluate_model(\"XGBoost + SMOTE\", y_test, xgb_smote_proba)\n",
    "all_results.append(xgb_smote_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0fc03",
   "metadata": {},
   "source": [
    "### Manually implementation: Classification Tree with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ee3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DecisionTree as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# find the best model\n",
    "max_depth_list = [5,10,15,20]\n",
    "min_sample_list = [5,10,15,20]\n",
    "loss_list = [\"Entropy\", \"Gini\"]\n",
    "best_evaluation_loss = float(\"inf\")\n",
    "best_max_depth = 0\n",
    "best_min_sample = 0\n",
    "best_lossfunc = None\n",
    "best_tree = None\n",
    "\n",
    "total_combinations = len(max_depth_list) * len(min_sample_list) * len(loss_list)\n",
    "with tqdm(total=total_combinations, desc=\"Grid Search\", unit=\"models\") as pbar:\n",
    "    for max_depth in max_depth_list:\n",
    "        for min_sample in min_sample_list:\n",
    "            for loss_function_name in loss_list:\n",
    "                pbar.set_postfix(max_depth=max_depth, min_sample=min_sample)\n",
    "                clstree = dt.ClassificationTree(loss_function=loss_function_name, leaf_value_estimator=\"most_common_vote\", max_depth=max_depth, min_sample=min_sample)\n",
    "                clstree.fit(X_train_smote, y_train_smote)\n",
    "                clstree.prune(X_val_transformed, y_val)\n",
    "                y_preds = clstree.predict(X_val_transformed)\n",
    "                current_evaluation_loss = np.sum(y_preds != y_val)\n",
    "                if current_evaluation_loss < best_evaluation_loss:\n",
    "                    best_evaluation_loss = current_evaluation_loss\n",
    "                    best_max_depth = max_depth \n",
    "                    best_min_sample = min_sample\n",
    "                    best_lossfunc = loss_function_name\n",
    "                    best_tree = clstree\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "print(f\"\\nBest parameters: max_depth={best_max_depth}, min_sample={best_min_sample}, loss_function = {best_lossfunc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41398315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on test set\n",
    "tree_preds = best_tree.predict(X_test_transformed)\n",
    "evaluate_model(\"Classificationtree\", y_test, tree_preds)\n",
    "all_results.append(tree_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f168ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with class weights (scale_pos_weight)\n",
    "xgb_weighted_pipeline = Pipeline([\n",
    "    (\"preprocessor\", clone(preprocessor)),\n",
    "    (\"classifier\", XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.15,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.5,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        scale_pos_weight=pos_weight,\n",
    "        tree_method=\"hist\",\n",
    "    )),\n",
    "])\n",
    "\n",
    "xgb_weighted_pipeline.fit(X_train, y_train)\n",
    "xgb_weighted_proba = xgb_weighted_pipeline.predict_proba(X_test)[:, 1]\n",
    "xgb_weighted_results = evaluate_model(\"XGBoost (weighted)\", y_test, xgb_weighted_proba)\n",
    "all_results.append(xgb_weighted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v3qpk0hha8l",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter Optimization\n",
    "\n",
    "Use `BayesSearchCV` from scikit-optimize to find optimal hyperparameters for each model. Bayesian optimization is more efficient than grid search as it uses past evaluation results to choose the next hyperparameters to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hfiwz8hhhsm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization imports\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Use stratified k-fold for imbalanced data\n",
    "bayes_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# SMOTE for handling class imbalance (applied only during training in CV)\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "\n",
    "# Store Bayesian optimization results\n",
    "bayes_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02wn3yqe1f0j",
   "metadata": {},
   "source": [
    "### XGBoost Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eitfq3gn96l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Bayesian Search with SMOTE\n",
    "xgb_search_spaces = {\n",
    "    'classifier__n_estimators': Integer(100, 1000),\n",
    "    'classifier__max_depth': Integer(3, 10),\n",
    "    'classifier__learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'classifier__subsample': Real(0.6, 1.0),\n",
    "    'classifier__colsample_bytree': Real(0.6, 1.0),\n",
    "    'classifier__gamma': Real(0, 5),\n",
    "    'classifier__min_child_weight': Integer(1, 10),\n",
    "    'classifier__reg_alpha': Real(1e-8, 10.0, prior='log-uniform'),\n",
    "    'classifier__reg_lambda': Real(1e-8, 10.0, prior='log-uniform'),\n",
    "}\n",
    "\n",
    "xgb_base_model = XGBClassifier(\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    tree_method='hist',\n",
    ")\n",
    "\n",
    "# Create pipeline with SMOTE and XGBoost\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('classifier', xgb_base_model)\n",
    "])\n",
    "\n",
    "xgb_bayes_search = BayesSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    search_spaces=xgb_search_spaces,\n",
    "    n_iter=50,  # Number of parameter settings to try\n",
    "    cv=bayes_cv,\n",
    "    scoring='roc_auc',  # Use ROC-AUC for imbalanced data\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Starting XGBoost Bayesian Optimization with SMOTE...\")\n",
    "xgb_bayes_search.fit(X_train_transformed, y_train)\n",
    "\n",
    "bayes_results['XGBoost'] = {\n",
    "    'best_params': xgb_bayes_search.best_params_,\n",
    "    'best_score': xgb_bayes_search.best_score_,\n",
    "    'best_estimator': xgb_bayes_search.best_estimator_,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XGBoost Bayesian Optimization Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best parameters: {xgb_bayes_search.best_params_}\")\n",
    "print(f\"Best CV ROC-AUC: {xgb_bayes_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brt84v6u5tb",
   "metadata": {},
   "source": [
    "### Bayesian Optimization Summary & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "euxdukf9hvr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Bayesian Optimization Results\n",
    "print(\"=\" * 80)\n",
    "print(\"BAYESIAN OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bayes_summary = []\n",
    "for model_name, results in bayes_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Best CV ROC-AUC: {results['best_score']:.4f}\")\n",
    "    print(f\"  Best Parameters: {results['best_params']}\")\n",
    "    bayes_summary.append({\n",
    "        'Model': model_name,\n",
    "        'Best CV ROC-AUC': results['best_score'],\n",
    "    })\n",
    "\n",
    "bayes_summary_df = pd.DataFrame(bayes_summary).sort_values('Best CV ROC-AUC', ascending=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL RANKING BY CV ROC-AUC:\")\n",
    "print(\"=\" * 80)\n",
    "print(bayes_summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v4cl4tsda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models on test set\n",
    "bayes_tuned_results = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATING BAYESIAN-TUNED MODELS ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, results in bayes_results.items():\n",
    "    best_model = results['best_estimator']\n",
    "    \n",
    "    # Get predictions\n",
    "    y_proba = best_model.predict_proba(X_test_transformed)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    tuned_results = evaluate_model(f\"{model_name} (Bayes-tuned)\", y_test, y_proba)\n",
    "    bayes_tuned_results.append(tuned_results)\n",
    "\n",
    "# Add to main results list for comparison\n",
    "all_results.extend(bayes_tuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93is9wwiw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned vs untuned models\n",
    "tuned_comparison = pd.DataFrame([\n",
    "    {k: v for k, v in r.items() if k not in [\"predictions\", \"probabilities\"]}\n",
    "    for r in bayes_tuned_results\n",
    "]).sort_values(\"f1\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BAYESIAN-TUNED MODELS COMPARISON (sorted by F1 score)\")\n",
    "print(\"=\" * 80)\n",
    "print(tuned_comparison.to_string(index=False))\n",
    "\n",
    "# Find best tuned model\n",
    "best_tuned_model = tuned_comparison.iloc[0][\"name\"]\n",
    "best_tuned_f1 = tuned_comparison.iloc[0][\"f1\"]\n",
    "print(f\"\\nBest Bayesian-tuned model: {best_tuned_model} (F1 = {best_tuned_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jnfufql5r3",
   "metadata": {},
   "source": [
    "### Reactive Rule Learner (RRL)\n",
    "\n",
    "RRL is an interpretable rule-based model that learns logical rules from data. It provides transparent decision-making through human-readable rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otxrk7hydn",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RRL Training using preprocessed and engineered features with hyperparameter search\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import importlib\n",
    "import rrl.models\n",
    "from rrl.models import RRL\n",
    "from rrl.utils import DBEncoder\n",
    "from itertools import product\n",
    "\n",
    "# Set device for RRL\n",
    "rrl_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        rrl_device = torch.device(\"mps\")\n",
    "except:\n",
    "    pass\n",
    "print(f\"Using device for RRL: {rrl_device}\", flush=True)\n",
    "\n",
    "# Use engineered features (same as other models)\n",
    "# Create feature info dataframe for DBEncoder from X_engineered\n",
    "print(\"Preparing engineered features for RRL...\", flush=True)\n",
    "f_list = []\n",
    "for col in X_engineered.columns:\n",
    "    if X_engineered[col].dtype == 'object':\n",
    "        f_list.append([col, 'discrete'])\n",
    "    else:\n",
    "        f_list.append([col, 'continuous'])\n",
    "\n",
    "f_df = pd.DataFrame(f_list)\n",
    "\n",
    "# Initialize DBEncoder with engineered features\n",
    "db_enc = DBEncoder(f_df, discrete=False)\n",
    "\n",
    "# Create target dataframe (DBEncoder expects DataFrame)\n",
    "y_train_df = y_train.to_frame(name='y')\n",
    "y_val_df = y_val.to_frame(name='y')\n",
    "y_test_df = y_test.to_frame(name='y')\n",
    "\n",
    "# Fit encoder on training data\n",
    "db_enc.fit(X_train, y_train_df)\n",
    "\n",
    "# Transform all splits\n",
    "X_rrl_train, y_rrl_train = db_enc.transform(X_train, y_train_df, normalized=True, keep_stat=True)\n",
    "X_rrl_val, y_rrl_val = db_enc.transform(X_val, y_val_df, normalized=True, keep_stat=False)\n",
    "X_rrl_test, y_rrl_test = db_enc.transform(X_test, y_test_df, normalized=True, keep_stat=False)\n",
    "\n",
    "print(f\"RRL Training samples: {X_rrl_train.shape[0]:,}\", flush=True)\n",
    "print(f\"RRL Validation samples: {X_rrl_val.shape[0]:,}\", flush=True)\n",
    "print(f\"RRL Test samples: {X_rrl_test.shape[0]:,}\", flush=True)\n",
    "\n",
    "# Create DataLoaders with optimized settings for speed\n",
    "print(\"Creating tensors...\", flush=True)\n",
    "rrl_batch_size = 256\n",
    "\n",
    "# FIX: Explicitly convert to float32 numpy arrays before creating tensors\n",
    "# DBEncoder.transform can return object dtype arrays which cause torch.tensor() to hang\n",
    "X_rrl_train = np.asarray(X_rrl_train, dtype=np.float32)\n",
    "X_rrl_val = np.asarray(X_rrl_val, dtype=np.float32)\n",
    "X_rrl_test = np.asarray(X_rrl_test, dtype=np.float32)\n",
    "y_rrl_train = np.asarray(y_rrl_train, dtype=np.float32)\n",
    "y_rrl_val = np.asarray(y_rrl_val, dtype=np.float32)\n",
    "y_rrl_test = np.asarray(y_rrl_test, dtype=np.float32)\n",
    "\n",
    "# Create tensors using torch.from_numpy (faster than torch.tensor for contiguous arrays)\n",
    "X_train_tensor = torch.from_numpy(X_rrl_train)\n",
    "y_train_tensor = torch.from_numpy(y_rrl_train)\n",
    "X_val_tensor = torch.from_numpy(X_rrl_val)\n",
    "y_val_tensor = torch.from_numpy(y_rrl_val)\n",
    "X_test_tensor = torch.from_numpy(X_rrl_test)\n",
    "y_test_tensor = torch.from_numpy(y_rrl_test)\n",
    "\n",
    "rrl_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "rrl_val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "rrl_test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "rrl_train_loader = DataLoader(rrl_train_dataset, batch_size=rrl_batch_size, shuffle=True)\n",
    "rrl_val_loader = DataLoader(rrl_val_dataset, batch_size=rrl_batch_size, shuffle=False)\n",
    "rrl_test_loader = DataLoader(rrl_test_dataset, batch_size=rrl_batch_size, shuffle=False)\n",
    "\n",
    "discrete_flen = db_enc.discrete_flen\n",
    "continuous_flen = db_enc.continuous_flen\n",
    "rrl_output_dim = y_rrl_train.shape[1]  # One-hot encoded length\n",
    "\n",
    "print(f\"Discrete features: {discrete_flen}, Continuous features: {continuous_flen}\", flush=True)\n",
    "print(f\"RRL output dimension: {rrl_output_dim}\", flush=True)\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.005, 0.01],\n",
    "    'temperature': [0.05, 0.1, 0.2],\n",
    "    'structure': [[16, 16, 8], [32, 32, 16], [64, 32, 16]]\n",
    "}\n",
    "\n",
    "best_val_f1 = -float('inf')\n",
    "best_params = None\n",
    "best_model_state = None\n",
    "best_dim_list = None\n",
    "\n",
    "print(\"Starting RRL hyperparameter search...\", flush=True)\n",
    "print(\"=\" * 60, flush=True)\n",
    "all_combos = list(product(param_grid['lr'], param_grid['temperature'], param_grid['structure']))\n",
    "print(f\"Testing {len(all_combos)} configurations\", flush=True)\n",
    "\n",
    "for i, (lr, temp, struct) in enumerate(all_combos, start=1):\n",
    "    current_dim_list = [(discrete_flen, continuous_flen)] + list(struct) + [rrl_output_dim]\n",
    "\n",
    "    candidate_model = RRL(\n",
    "        dim_list=current_dim_list,\n",
    "        device=rrl_device,\n",
    "        use_not=True,\n",
    "        is_rank0=True,\n",
    "        save_best=False,\n",
    "        distributed=False,\n",
    "        use_skip=False,\n",
    "        save_path=MODELS_DIR / \"rrl_search_tmp.pth\",\n",
    "        temperature=temp\n",
    "    )\n",
    "\n",
    "    # Use fewer epochs during search for speed\n",
    "    candidate_model.train_model(\n",
    "        data_loader=rrl_train_loader,\n",
    "        valid_loader=rrl_val_loader,\n",
    "        epoch=30,\n",
    "        lr=lr,\n",
    "        weight_decay=1e-5,\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "    _, val_f1 = candidate_model.test(\n",
    "        test_loader=rrl_val_loader,\n",
    "        set_name='Validation',\n",
    "        show_progress=False\n",
    "    )\n",
    "\n",
    "    print(f\"[{i}/{len(all_combos)}] lr={lr}, temp={temp}, struct={struct} -> Val F1: {val_f1:.4f}\", flush=True)\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_params = {'lr': lr, 'temperature': temp, 'structure': list(struct)}\n",
    "        best_dim_list = current_dim_list\n",
    "        best_model_state = {k: v.detach().cpu() for k, v in candidate_model.net.state_dict().items()}\n",
    "\n",
    "if best_model_state is None:\n",
    "    raise RuntimeError(\"Hyperparameter search did not produce a valid model.\")\n",
    "\n",
    "print(\"\" + \"=\" * 60, flush=True)\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best validation F1: {best_val_f1:.4f}\")\n",
    "print(\"=\" * 60, flush=True)\n",
    "\n",
    "rrl_model_path = MODELS_DIR / 'rrl_bank_model.pth'\n",
    "rrl_log_path = MODELS_DIR / 'rrl_bank_log.txt'\n",
    "\n",
    "# Rebuild best model and save checkpoint\n",
    "rrl_model = RRL(\n",
    "    dim_list=best_dim_list,\n",
    "    device=rrl_device,\n",
    "    use_not=True,\n",
    "    is_rank0=True,\n",
    "    distributed=False,\n",
    "    save_best=False,\n",
    "    use_skip=False,\n",
    "    temperature=best_params['temperature']\n",
    ")\n",
    "rrl_model.net.load_state_dict(best_model_state)\n",
    "rrl_model.net.eval()\n",
    "\n",
    "best_rrl_args = {\n",
    "    'dim_list': best_dim_list,\n",
    "    'use_not': True,\n",
    "    'use_skip': False,\n",
    "    'estimated_grad': False,\n",
    "    'use_nlaf': False,\n",
    "    'alpha': 0.999,\n",
    "    'beta': 8,\n",
    "    'gamma': 1,\n",
    "    'temperature': best_params['temperature']\n",
    "}\n",
    "\n",
    "torch.save({'model_state_dict': best_model_state, 'rrl_args': best_rrl_args}, rrl_model_path)\n",
    "print(f\"Saved best RRL model to {rrl_model_path}\", flush=True)\n",
    "\n",
    "# Test best model\n",
    "print(\"Testing best RRL model...\", flush=True)\n",
    "rrl_model.test(test_loader=rrl_test_loader, set_name='Test', show_progress=True)\n",
    "\n",
    "# Print Rules\n",
    "print(\"Generating rules...\", flush=True)\n",
    "rrl_rules_path = MODELS_DIR / 'rrl_bank_rules.txt'\n",
    "with open(rrl_rules_path, 'w') as f:\n",
    "    rrl_model.rule_print(db_enc.X_fname, db_enc.y_fname, rrl_train_loader, file=f, mean=db_enc.mean, std=db_enc.std)\n",
    "\n",
    "print(f\"RRL Model saved to {rrl_model_path}\", flush=True)\n",
    "print(f\"RRL Rules saved to {rrl_rules_path}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liyf90z1f",
   "metadata": {},
   "source": [
    "### RRL Evaluation\n",
    "\n",
    "Load the trained RRL model and evaluate using the standard `evaluate_model` function for comparison with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zmtq1ndr1cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm as tqdm_auto\n",
    "\n",
    "print(f\"Loading RRL model from {rrl_model_path}...\")\n",
    "\n",
    "if not rrl_model_path.exists():\n",
    "    print(f\"Error: Model file not found at {rrl_model_path}\")\n",
    "else:\n",
    "    checkpoint = torch.load(rrl_model_path, map_location=rrl_device, weights_only=False)\n",
    "    saved_args = checkpoint['rrl_args']\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    # Re-instantiate RRL model with saved arguments\n",
    "    rrl_eval = RRL(\n",
    "        dim_list=saved_args['dim_list'],\n",
    "        device=rrl_device,\n",
    "        use_not=saved_args['use_not'],\n",
    "        use_skip=saved_args.get('use_skip', False),\n",
    "        estimated_grad=saved_args.get('estimated_grad', False),\n",
    "        use_nlaf=saved_args.get('use_nlaf', False),\n",
    "        alpha=saved_args.get('alpha', 0.999),\n",
    "        beta=saved_args.get('beta', 8),\n",
    "        gamma=saved_args.get('gamma', 1),\n",
    "        distributed=False,\n",
    "        is_rank0=True\n",
    "    )\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for k, v in model_state_dict.items():\n",
    "        name = k[7:] if k.startswith('module.') else k\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    rrl_eval.net.load_state_dict(new_state_dict)\n",
    "    rrl_eval.net.eval()\n",
    "    print(\"RRL Model loaded successfully.\")\n",
    "\n",
    "    rrl_y_proba_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm_auto(rrl_test_loader, desc=\"Generating predictions\", unit=\"batch\"):\n",
    "            X_batch = X_batch.to(rrl_device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = rrl_eval.net(X_batch)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            rrl_y_proba_list.extend(probs[:, 1].cpu().numpy())  # Probability of class 1 (yes)\n",
    "\n",
    "    rrl_y_proba = np.array(rrl_y_proba_list)\n",
    "\n",
    "    rrl_results = evaluate_model(\"RRL\", y_test.values, rrl_y_proba)\n",
    "    all_results.append(rrl_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lw0yjuc82h",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare all models across key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876rfj9rzsw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {k: v for k, v in r.items() if k not in [\"predictions\", \"probabilities\"]}\n",
    "    for r in all_results\n",
    "]).sort_values(\"f1\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON (sorted by F1 score)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df.iloc[0][\"name\"]\n",
    "best_f1 = comparison_df.iloc[0][\"f1\"]\n",
    "print(f\"\\nBest model: {best_model_name} (F1 = {best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oltxqc5nzro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "metrics_to_plot = [\"f1\", \"precision\", \"recall\"]\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(comparison_df)))\n",
    "\n",
    "for ax, metric in zip(axes, metrics_to_plot):\n",
    "    sorted_df = comparison_df.sort_values(metric, ascending=True)\n",
    "    bars = ax.barh(sorted_df[\"name\"], sorted_df[metric], color=colors)\n",
    "    ax.set_xlabel(metric.capitalize())\n",
    "    ax.set_title(f\"{metric.capitalize()} by Model\")\n",
    "    ax.set_xlim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, sorted_df[metric]):\n",
    "        ax.text(val + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                f\"{val:.3f}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_plot_path = PLOTS_DIR / \"model_comparison.png\"\n",
    "plt.savefig(comparison_plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {comparison_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823720cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, title: str, ax=None):\n",
    "    \"\"\"Plot a confusion matrix with annotations.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "        xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"]\n",
    "    )\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Plot confusion matrices for all models in a grid\n",
    "n_models = len(all_results)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    title = f\"{result['name']}\\n(thr={result['threshold']:.3f})\"\n",
    "    plot_confusion_matrix(y_test, result[\"predictions\"], title, ax=axes[idx])\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(all_results), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "cm_plot_path = PLOTS_DIR / \"confusion_matrices.png\"\n",
    "plt.savefig(cm_plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {cm_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7cb9e",
   "metadata": {},
   "source": [
    "## Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map model names to pipelines\n",
    "pipelines = {\n",
    "    \"Logistic Regression\": log_reg_pipeline,\n",
    "    \"XGBoost (weighted)\": xgb_weighted_pipeline,\n",
    "    \"XGBoost + SMOTE\": xgb_smote_pipeline,\n",
    "    \"Random Forest + SMOTE\": rf_smote_pipeline,\n",
    "}\n",
    "\n",
    "# Save all models with their metadata\n",
    "for result in all_results:\n",
    "    model_name = result[\"name\"]\n",
    "    safe_name = model_name.lower().replace(\" \", \"_\").replace(\"+\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    # Skip RRL - it's already saved as a PyTorch model (.pth) earlier\n",
    "    if model_name == \"RRL\":\n",
    "        print(f\"Skipped: {model_name} (already saved as {rrl_model_path})\")\n",
    "        continue\n",
    "    \n",
    "    pipeline = pipelines[model_name]\n",
    "    \n",
    "    # Handle wrapper objects vs sklearn pipelines\n",
    "    if hasattr(pipeline, 'named_steps'):\n",
    "        model_to_save = pipeline\n",
    "    else:\n",
    "        # For wrapper objects, save the components\n",
    "        model_to_save = {\n",
    "            \"preprocessor\": pipeline.preprocessor,\n",
    "            \"model\": pipeline.model,\n",
    "            \"is_wrapper\": True\n",
    "        }\n",
    "    \n",
    "    model_data = {\n",
    "        \"pipeline\": model_to_save,\n",
    "        \"threshold\": result[\"threshold\"],\n",
    "        \"metrics\": {k: v for k, v in result.items() if k not in [\"predictions\", \"probabilities\"]},\n",
    "    }\n",
    "    \n",
    "    model_path = MODELS_DIR / f\"{safe_name}.pkl\"\n",
    "    joblib.dump(model_data, model_path)\n",
    "    print(f\"Saved: {model_path}\")\n",
    "\n",
    "print(f\"\\nBest model by F1 score: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10e38c",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "Export predictions from all models for analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac88c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile predictions from all models\n",
    "predictions_df = pd.DataFrame({\"actual\": y_test})\n",
    "\n",
    "for result in all_results:\n",
    "    safe_name = result[\"name\"].lower().replace(\" \", \"_\").replace(\"+\", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    predictions_df[f\"{safe_name}_prob\"] = result[\"probabilities\"]\n",
    "    predictions_df[f\"{safe_name}_pred\"] = result[\"predictions\"]\n",
    "\n",
    "# Save predictions\n",
    "predictions_path = PREDICTIONS_DIR / \"classification_predictions.csv\"\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "print(f\"Saved predictions: {predictions_path}\")\n",
    "print(f\"Shape: {predictions_df.shape}\")\n",
    "print(f\"\\nColumns: {list(predictions_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53021071",
   "metadata": {},
   "source": [
    "## Model Interpretability\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to understand feature importance for the best tree-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ea3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best tree-based model for SHAP analysis\n",
    "tree_models = comparison_df[comparison_df[\"name\"].str.contains(\"XGBoost|Random Forest\")]\n",
    "best_tree_model = tree_models.iloc[0][\"name\"]\n",
    "\n",
    "pipeline_map = {\n",
    "    \"XGBoost (weighted)\": xgb_weighted_pipeline,\n",
    "    \"XGBoost + SMOTE\": xgb_smote_pipeline,\n",
    "    \"Random Forest + SMOTE\": rf_smote_pipeline,\n",
    "}\n",
    "\n",
    "shap_pipeline = pipeline_map[best_tree_model]\n",
    "print(f\"Generating SHAP values for: {best_tree_model}\")\n",
    "\n",
    "# Get feature names and classifier based on pipeline type\n",
    "if hasattr(shap_pipeline, 'named_steps'):\n",
    "    # Standard sklearn pipeline\n",
    "    feature_names = shap_pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "    X_test_transformed = shap_pipeline.named_steps[\"preprocessor\"].transform(X_test)\n",
    "    classifier = shap_pipeline.named_steps[\"classifier\"]\n",
    "else:\n",
    "    # Wrapper object\n",
    "    feature_names = shap_pipeline.preprocessor.get_feature_names_out()\n",
    "    X_test_transformed = shap_pipeline.preprocessor.transform(X_test)\n",
    "    classifier = shap_pipeline.model\n",
    "\n",
    "# Sample for faster computation\n",
    "sample_size = min(1000, X_test_transformed.shape[0])\n",
    "sample_idx = np.random.RandomState(RANDOM_STATE).choice(\n",
    "    X_test_transformed.shape[0], sample_size, replace=False\n",
    ")\n",
    "X_sample = X_test_transformed[sample_idx]\n",
    "\n",
    "# Convert sparse matrix to dense if needed\n",
    "if hasattr(X_sample, \"toarray\"):\n",
    "    X_sample = X_sample.toarray()\n",
    "\n",
    "# Create SHAP explainer and compute values\n",
    "explainer = shap.TreeExplainer(classifier)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Handle binary classification output format\n",
    "# Some models return list [neg_class, pos_class], others return single array\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_display = shap_values[1]  # Use positive class\n",
    "else:\n",
    "    shap_values_display = shap_values\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values_display.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xtll6lewh0i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary (bar chart)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values_display,\n",
    "    X_sample,\n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    max_display=15,\n",
    "    show=False,\n",
    ")\n",
    "plt.title(f\"Feature Importance - {best_tree_model}\")\n",
    "plt.tight_layout()\n",
    "shap_bar_path = PLOTS_DIR / \"shap_feature_importance.png\"\n",
    "plt.savefig(shap_bar_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {shap_bar_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vvdaf4txmmj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary (beeswarm - shows feature value impact)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values_display,\n",
    "    X_sample,\n",
    "    feature_names=feature_names,\n",
    "    max_display=15,\n",
    "    show=False,\n",
    ")\n",
    "plt.title(f\"SHAP Value Distribution - {best_tree_model}\")\n",
    "plt.tight_layout()\n",
    "shap_beeswarm_path = PLOTS_DIR / \"shap_beeswarm.png\"\n",
    "plt.savefig(shap_beeswarm_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {shap_beeswarm_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c3d546",
   "metadata": {},
   "source": [
    "\n",
    "# Bank Marketing Campaign Classification\n",
    "\n",
    "Improved pipeline using one-hot encoding, stratified splits, and tuned gradient boosting to lift recall/F1 on the term deposit prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42effcd6",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ee09a",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    recall_score,\n    precision_score,\n    roc_auc_score,\n    precision_recall_curve,\n)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport shap\nimport joblib\n\nsns.set(style=\"whitegrid\")"
  },
  {
   "cell_type": "markdown",
   "id": "c2608781",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6cddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resolve project paths and organize outputs\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"datasets\").exists() and (PROJECT_ROOT.parent / \"datasets\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if not (PROJECT_ROOT / \"datasets\").exists():\n",
    "    raise FileNotFoundError(\"Could not locate datasets folder. Run from project root or src directory.\")\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"datasets\" / \"bank-marketing\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "PLOTS_DIR = OUTPUT_DIR / \"plots\"\n",
    "MODELS_DIR = OUTPUT_DIR / \"models\"\n",
    "PREDICTIONS_DIR = OUTPUT_DIR / \"predictions\"\n",
    "for path in [OUTPUT_DIR, PLOTS_DIR, MODELS_DIR, PREDICTIONS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Outputs directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d162f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45,211 rows and 17 columns\n",
      "y\n",
      "no     0.883015\n",
      "yes    0.116985\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = DATA_DIR / \"bank-full.csv\"\n",
    "df = pd.read_csv(data_path, sep=\";\")\n",
    "print(f\"Loaded {df.shape[0]:,} rows and {df.shape[1]} columns from {data_path}\")\n",
    "print(df[\"y\"].value_counts(normalize=True).rename(\"proportion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9b859",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Preprocessing\n",
    "\n",
    "- Remove extreme outliers on a few numeric fields.\n",
    "- Separate categorical vs numeric columns for proper encoding.\n",
    "- Stratified train/test split to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8366f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After outlier filtering: 42,338 rows remain.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for col in [\"balance\", \"age\", \"duration\", \"campaign\"]:\n",
    "    z_scores = np.abs(df[col] - df[col].mean()) / df[col].std()\n",
    "    df = df[z_scores < 3]\n",
    "\n",
    "print(f\"After outlier filtering: {df.shape[0]:,} rows remain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d223511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive class weight (train): 8.58\n",
      "Train size: 33870, Test size: 8468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop(columns=\"y\")\n",
    "y = df[\"y\"].map({\"yes\": 1, \"no\": 0})\n",
    "\n",
    "categorical_cols = X.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "preprocess_template = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"numeric\", StandardScaler(with_mean=False), numeric_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
    "print(f\"Positive class weight (train): {pos_weight:.2f}\")\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gd8khc2c43d",
   "source": "### Feature Engineering\n\nCreate domain-specific features that might improve predictive power.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gkh9jw5auvi",
   "source": "# Create engineered features\ndef engineer_features(df):\n    \"\"\"Add domain-specific engineered features\"\"\"\n    df = df.copy()\n    \n    # Contact frequency features\n    df['contact_per_campaign'] = df['campaign'] / (df['previous'] + 1)\n    df['days_since_last'] = df['pdays'].apply(lambda x: 999 if x == -1 else x)\n    df['was_contacted_before'] = (df['pdays'] != -1).astype(int)\n    \n    # Financial features\n    df['balance_to_age_ratio'] = df['balance'] / (df['age'] + 1)\n    df['high_balance'] = (df['balance'] > df['balance'].median()).astype(int)\n    \n    # Call duration features (highly predictive but use cautiously)\n    df['duration_minutes'] = df['duration'] / 60\n    df['long_call'] = (df['duration'] > 300).astype(int)  # > 5 minutes\n    \n    # Time-based features\n    if 'day' in df.columns:\n        df['is_month_start'] = (df['day'] <= 10).astype(int)\n        df['is_month_end'] = (df['day'] >= 20).astype(int)\n    \n    return df\n\n# Apply feature engineering\nX_train_eng = engineer_features(X_train)\nX_test_eng = engineer_features(X_test)\n\nprint(f\"Features after engineering: {X_train_eng.shape[1]}\")\nprint(f\"New features: {set(X_train_eng.columns) - set(X_train.columns)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h6i9yq5dnmi",
   "source": "# Update preprocessing for engineered features\ncategorical_cols_eng = X_train_eng.select_dtypes(exclude=[\"number\"]).columns.tolist()\nnumeric_cols_eng = [col for col in X_train_eng.columns if col not in categorical_cols_eng]\n\npreprocess_eng = ColumnTransformer(\n    transformers=[\n        (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols_eng),\n        (\"numeric\", StandardScaler(), numeric_cols_eng),\n    ]\n)\n\nprint(f\"Categorical features: {len(categorical_cols_eng)}\")\nprint(f\"Numeric features: {len(numeric_cols_eng)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa8f563",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7a8a8",
   "metadata": {},
   "outputs": [],
   "source": "def find_best_threshold(probs, y_true, search=np.linspace(0.1, 0.9, 81)):\n    \"\"\"Find threshold that maximizes F1 score\"\"\"\n    scores = [(thr, f1_score(y_true, (probs >= thr).astype(int))) for thr in search]\n    return max(scores, key=lambda x: x[1])\n\ndef find_best_threshold_pr_curve(probs, y_true):\n    \"\"\"Find threshold using precision-recall curve\"\"\"\n    precision, recall, thresholds = precision_recall_curve(y_true, probs)\n    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n    best_idx = np.argmax(f1_scores)\n    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n    return best_threshold, f1_scores[best_idx]\n\ndef evaluate(name, probs, y_true, threshold=None, use_pr_curve=True):\n    \"\"\"Evaluate model with comprehensive metrics\"\"\"\n    if threshold is None:\n        if use_pr_curve:\n            threshold, _ = find_best_threshold_pr_curve(probs, y_true)\n        else:\n            threshold, _ = find_best_threshold(probs, y_true)\n    \n    preds = (probs >= threshold).astype(int)\n    acc = accuracy_score(y_true, preds)\n    f1 = f1_score(y_true, preds)\n    precision = precision_score(y_true, preds)\n    recall = recall_score(y_true, preds)\n    roc_auc = roc_auc_score(y_true, probs)\n    \n    print(f\"\\n{name} @ threshold {threshold:.3f}\")\n    print(f\"  Accuracy:  {acc:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall:    {recall:.4f}\")\n    print(f\"  F1:        {f1:.4f}\")\n    print(f\"  ROC AUC:   {roc_auc:.4f}\")\n    \n    return preds, threshold, {\n        'name': name,\n        'threshold': threshold,\n        'accuracy': acc,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'roc_auc': roc_auc\n    }"
  },
  {
   "cell_type": "markdown",
   "id": "23ac5eba",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression (one-hot + class weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03860180",
   "metadata": {},
   "outputs": [],
   "source": "log_pipeline = Pipeline(\n    steps=[\n        (\"preprocess\", clone(preprocess_eng)),\n        (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=0.1)),\n    ]\n)\n\nlog_pipeline.fit(X_train_eng, y_train)\nlog_probs = log_pipeline.predict_proba(X_test_eng)[:, 1]\nlog_preds, log_threshold, log_metrics = evaluate(\"Logistic Regression\", log_probs, y_test)"
  },
  {
   "cell_type": "markdown",
   "id": "64f16a8f",
   "metadata": {},
   "source": [
    "## 5. Tuned XGBoost (best current performer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w71qi00vuxe",
   "source": "## 5a. XGBoost with SMOTE (to handle class imbalance)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "h6bhlqiwemw",
   "source": "# XGBoost with SMOTE oversampling\nsmote_xgb_clf = XGBClassifier(\n    n_estimators=400,\n    learning_rate=0.03,\n    max_depth=5,\n    min_child_weight=3,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    gamma=0.15,\n    reg_alpha=0.1,\n    reg_lambda=1.5,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    tree_method=\"hist\",\n)\n\nsmote_xgb_pipeline = ImbPipeline(\n    steps=[\n        (\"preprocess\", clone(preprocess_eng)),\n        (\"smote\", SMOTE(random_state=42, k_neighbors=5)),\n        (\"model\", smote_xgb_clf),\n    ]\n)\n\nsmote_xgb_pipeline.fit(X_train_eng, y_train)\nsmote_xgb_probs = smote_xgb_pipeline.predict_proba(X_test_eng)[:, 1]\nsmote_xgb_preds, smote_xgb_threshold, smote_xgb_metrics = evaluate(\"XGBoost + SMOTE\", smote_xgb_probs, y_test)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nwbk8i29n6",
   "source": "## 5b. Random Forest with SMOTE",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w3ew4vn7l4",
   "source": "# Random Forest with SMOTE\nrf_clf = RandomForestClassifier(\n    n_estimators=300,\n    max_depth=12,\n    min_samples_split=10,\n    min_samples_leaf=4,\n    max_features='sqrt',\n    random_state=42,\n    n_jobs=-1,\n)\n\nsmote_rf_pipeline = ImbPipeline(\n    steps=[\n        (\"preprocess\", clone(preprocess_eng)),\n        (\"smote\", SMOTE(random_state=42, k_neighbors=5)),\n        (\"model\", rf_clf),\n    ]\n)\n\nsmote_rf_pipeline.fit(X_train_eng, y_train)\nsmote_rf_probs = smote_rf_pipeline.predict_proba(X_test_eng)[:, 1]\nsmote_rf_preds, smote_rf_threshold, smote_rf_metrics = evaluate(\"Random Forest + SMOTE\", smote_rf_probs, y_test)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bax69h1xs5r",
   "source": "## 5c. LightGBM with SMOTE (often best for imbalanced data)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wlkx26l2f9",
   "source": "# LightGBM with SMOTE (often superior for imbalanced datasets)\nlgbm_clf = LGBMClassifier(\n    n_estimators=500,\n    learning_rate=0.02,\n    max_depth=6,\n    num_leaves=31,\n    min_child_samples=20,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42,\n    verbose=-1,\n)\n\nsmote_lgbm_pipeline = ImbPipeline(\n    steps=[\n        (\"preprocess\", clone(preprocess_eng)),\n        (\"smote\", SMOTE(random_state=42, k_neighbors=5)),\n        (\"model\", lgbm_clf),\n    ]\n)\n\nsmote_lgbm_pipeline.fit(X_train_eng, y_train)\nsmote_lgbm_probs = smote_lgbm_pipeline.predict_proba(X_test_eng)[:, 1]\nsmote_lgbm_preds, smote_lgbm_threshold, smote_lgbm_metrics = evaluate(\"LightGBM + SMOTE\", smote_lgbm_probs, y_test)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f168ae8",
   "metadata": {},
   "outputs": [],
   "source": "xgb_clf = XGBClassifier(\n    n_estimators=400,\n    learning_rate=0.03,\n    max_depth=5,\n    min_child_weight=3,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    gamma=0.15,\n    reg_alpha=0.1,\n    reg_lambda=1.5,\n    objective=\"binary:logistic\",\n    eval_metric=\"logloss\",\n    random_state=42,\n    scale_pos_weight=pos_weight,\n    tree_method=\"hist\",\n)\n\nxgb_pipeline = Pipeline(\n    steps=[\n        (\"preprocess\", clone(preprocess_eng)),\n        (\"model\", xgb_clf),\n    ]\n)\n\nxgb_pipeline.fit(X_train_eng, y_train)\nxgb_probs = xgb_pipeline.predict_proba(X_test_eng)[:, 1]\nxgb_preds, xgb_threshold, xgb_metrics = evaluate(\"XGBoost (tuned)\", xgb_probs, y_test)"
  },
  {
   "cell_type": "markdown",
   "id": "7891bd8c",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lw0yjuc82h",
   "source": "## 5d. Model Comparison",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "876rfj9rzsw",
   "source": "# Compare all models\nall_metrics = [log_metrics, xgb_metrics, smote_xgb_metrics, smote_rf_metrics, smote_lgbm_metrics]\ncomparison_df = pd.DataFrame(all_metrics)\ncomparison_df = comparison_df.sort_values('f1', ascending=False)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL COMPARISON (sorted by F1 score)\")\nprint(\"=\"*80)\nprint(comparison_df.to_string(index=False))\nprint(\"=\"*80)\n\n# Visualize comparison\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nmetrics_to_plot = ['f1', 'precision', 'recall']\nfor idx, metric in enumerate(metrics_to_plot):\n    ax = axes[idx]\n    comparison_df_sorted = comparison_df.sort_values(metric, ascending=True)\n    ax.barh(comparison_df_sorted['name'], comparison_df_sorted[metric])\n    ax.set_xlabel(metric.capitalize())\n    ax.set_title(f'{metric.capitalize()} Comparison')\n    ax.set_xlim([0, 1])\n    \nplt.tight_layout()\ncomparison_plot_path = PLOTS_DIR / \"model_comparison.png\"\nplt.savefig(comparison_plot_path)\nplt.show()\nprint(f\"\\nSaved comparison plot to {comparison_plot_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823720cd",
   "metadata": {},
   "outputs": [],
   "source": "def plot_confusion(y_true, y_pred, title):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(title)\n    plt.tight_layout()\n    outfile = PLOTS_DIR / f\"{title.replace(' ', '_').lower()}_confusion_matrix.png\"\n    plt.savefig(outfile)\n    plt.show()\n    print(f\"Saved confusion matrix to {outfile}\")\n\n# Plot confusion matrices for all models\nmodel_results = [\n    (\"Logistic Regression\", log_preds, log_threshold),\n    (\"XGBoost\", xgb_preds, xgb_threshold),\n    (\"XGBoost + SMOTE\", smote_xgb_preds, smote_xgb_threshold),\n    (\"Random Forest + SMOTE\", smote_rf_preds, smote_rf_threshold),\n    (\"LightGBM + SMOTE\", smote_lgbm_preds, smote_lgbm_threshold),\n]\n\nfor name, preds, threshold in model_results:\n    plot_confusion(y_test, preds, f\"{name} (thr={threshold:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "06d7cb9e",
   "metadata": {},
   "source": [
    "## 7. Persist Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a0d90",
   "metadata": {},
   "outputs": [],
   "source": "# Save all models\nmodels_to_save = {\n    \"logistic_regression\": {\"pipeline\": log_pipeline, \"threshold\": log_threshold},\n    \"xgboost\": {\"pipeline\": xgb_pipeline, \"threshold\": xgb_threshold},\n    \"xgboost_smote\": {\"pipeline\": smote_xgb_pipeline, \"threshold\": smote_xgb_threshold},\n    \"random_forest_smote\": {\"pipeline\": smote_rf_pipeline, \"threshold\": smote_rf_threshold},\n    \"lightgbm_smote\": {\"pipeline\": smote_lgbm_pipeline, \"threshold\": smote_lgbm_threshold},\n}\n\nfor model_name, model_data in models_to_save.items():\n    model_path = MODELS_DIR / f\"{model_name}.pkl\"\n    joblib.dump(model_data, model_path)\n    print(f\"Saved {model_name} to {model_path}\")\n\n# Identify and save best model\nbest_model_name = comparison_df.iloc[0]['name']\nprint(f\"\\nBest model by F1 score: {best_model_name}\")\nprint(f\"F1 Score: {comparison_df.iloc[0]['f1']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "6d10e38c",
   "metadata": {},
   "source": [
    "## 8. Generate Predictions and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac88c89",
   "metadata": {},
   "outputs": [],
   "source": "# Generate predictions from all models for comparison\npred_output = pd.DataFrame({\n    'actual': y_test,\n    'log_reg_prob': log_probs,\n    'log_reg_pred': log_preds,\n    'xgb_prob': xgb_probs,\n    'xgb_pred': xgb_preds,\n    'xgb_smote_prob': smote_xgb_probs,\n    'xgb_smote_pred': smote_xgb_preds,\n    'rf_smote_prob': smote_rf_probs,\n    'rf_smote_pred': smote_rf_preds,\n    'lgbm_smote_prob': smote_lgbm_probs,\n    'lgbm_smote_pred': smote_lgbm_preds,\n})\n\npredictions_path = PREDICTIONS_DIR / \"classification_predictions.csv\"\npred_output.to_csv(predictions_path, index=False)\nprint(f\"Saved predictions to {predictions_path}\")\nprint(f\"Prediction file shape: {pred_output.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "53021071",
   "metadata": {},
   "source": [
    "## 9. Model Interpretability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ea3be",
   "metadata": {},
   "outputs": [],
   "source": "# SHAP analysis for the best performing SMOTE model (LightGBM or XGBoost)\n# Use LightGBM SMOTE if it has the best F1, otherwise use XGBoost SMOTE\nbest_tree_model_name = comparison_df[comparison_df['name'].str.contains('SMOTE')].iloc[0]['name']\n\nif 'LightGBM' in best_tree_model_name:\n    shap_pipeline = smote_lgbm_pipeline\n    model_label = \"LightGBM + SMOTE\"\nelif 'XGBoost' in best_tree_model_name:\n    shap_pipeline = smote_xgb_pipeline\n    model_label = \"XGBoost + SMOTE\"\nelse:\n    shap_pipeline = smote_rf_pipeline\n    model_label = \"Random Forest + SMOTE\"\n\nprint(f\"Generating SHAP values for: {model_label}\")\n\nfeature_names = shap_pipeline.named_steps[\"preprocess\"].get_feature_names_out()\nX_test_encoded = shap_pipeline.named_steps[\"preprocess\"].transform(X_test_eng)\n\nsample_size = min(1000, X_test_encoded.shape[0])\nsample_indices = np.random.RandomState(42).choice(\n    X_test_encoded.shape[0], sample_size, replace=False\n)\nX_sample = X_test_encoded[sample_indices]\n\nif hasattr(X_sample, \"toarray\"):\n    X_sample = X_sample.toarray()\n\nexplainer = shap.TreeExplainer(shap_pipeline.named_steps[\"model\"])\nshap_values = explainer.shap_values(X_sample)\n\nshap.summary_plot(\n    shap_values,\n    feature_names=feature_names,\n    plot_type=\"bar\",\n    max_display=15,\n    show=False,\n)\nplt.tight_layout()\nshap_plot_path = PLOTS_DIR / f\"{model_label.lower().replace(' ', '_')}_shap_summary_bar.png\"\nplt.savefig(shap_plot_path, bbox_inches=\"tight\")\nplt.close()\nprint(f\"Saved SHAP summary plot to {shap_plot_path}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}